# MLOps Layer for LegalAid Connect

This directory contains a **non-intrusive MLOps pipeline** for the LegalAid Connect matchmaking model. It operates independently from the production system and provides:

âœ… **Experiment tracking** (MLflow)  
âœ… **Data versioning** (DVC)  
âœ… **Automated retraining**  
âœ… **Model monitoring** (EvidentlyAI)  
âœ… **Performance evaluation**  
âœ… **CI/CD automation** (GitHub Actions)

---

## ğŸ“ Directory Structure

```
mlops/
â”œâ”€â”€ data/                          # Dataset snapshots (versioned with DVC)
â”‚   â””â”€â”€ matchmaking_dataset.csv    # Training data copy
â”œâ”€â”€ models/                        # Versioned model artifacts
â”‚   â””â”€â”€ v<timestamp>/              # Candidate models (if improved)
â”œâ”€â”€ tracking/                      # MLflow experiment logs
â”‚   â””â”€â”€ mlruns/                    # MLflow backend store
â”œâ”€â”€ reports/                       # Evaluation and drift reports
â”‚   â”œâ”€â”€ evaluation_*.txt           # Performance reports
â”‚   â””â”€â”€ data_drift_report_*.html   # Evidently drift reports
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ utils.py                   # Shared utilities
â”‚   â”œâ”€â”€ retrain_model.py           # Retraining pipeline
â”‚   â”œâ”€â”€ evaluate_model.py          # Model evaluation
â”‚   â””â”€â”€ monitor_model.py           # Data drift monitoring
â”œâ”€â”€ mlflow_setup.py                # MLflow configuration
â”œâ”€â”€ dvc.yaml                       # DVC pipeline definition
â””â”€â”€ requirements.txt               # MLOps dependencies
```

---

## ğŸš€ Quick Start

### 1. Install Dependencies

```powershell
pip install -r mlops/requirements.txt
```

### 2. Initialize DVC (Optional - for dataset versioning)

```powershell
cd mlops
dvc init
dvc add data/matchmaking_dataset.csv
git add data/matchmaking_dataset.csv.dvc data/.gitignore
git commit -m "Track dataset with DVC"
cd ..
```

### 3. Run MLflow UI (View Experiments)

```powershell
mlflow ui --backend-store-uri file:./mlops/tracking --port 5001
```

Then open: **http://localhost:5001**

---

## ğŸ“Š Core Workflows

### A. Retrain Model

Trains a candidate model and compares it to production:

```powershell
python mlops/scripts/retrain_model.py
```

**What it does:**
- Loads production model, encoder, and scaler from `ml_model/`
- Trains a new candidate model on `mlops/data/matchmaking_dataset.csv`
- Compares accuracy: candidate vs. production
- If improved:
  - Saves to `mlops/models/v<timestamp>/lawyer_match_model.h5`
  - Logs to MLflow with metrics and artifacts
- If not improved: logs metrics only (no model saved)

**Output:**
- `mlops/tracking/last_metrics.json` - Latest comparison
- `mlops/models/v<timestamp>/` - Candidate model (if better)
- MLflow run with full metrics

---

### B. Evaluate Production Model

Evaluates the current production model:

```powershell
python mlops/scripts/evaluate_model.py
```

**What it does:**
- Loads production model from `ml_model/lawyer_match_model.h5`
- Evaluates on test split of `mlops/data/matchmaking_dataset.csv`
- Logs metrics to MLflow
- Generates text report

**Output:**
- `mlops/reports/evaluation_<timestamp>.txt`
- `mlops/tracking/eval_<timestamp>.json`
- MLflow run with evaluation metrics

---

### C. Monitor Data Drift

Detects data drift using EvidentlyAI:

```powershell
python mlops/scripts/monitor_model.py
```

**What it does:**
- Reference data: `mlops/data/matchmaking_dataset.csv` (training snapshot)
- Current data: `mlops/data/current_data.csv` (if exists) or sample
- Generates HTML drift report with visualizations

**Output:**
- `mlops/reports/data_drift_report_<timestamp>.html`

**To monitor live data:**
1. Export recent predictions to `mlops/data/current_data.csv` (same schema)
2. Run monitor script
3. Open HTML report to view drift metrics

---

## ğŸ”„ DVC Pipeline (Optional)

Run the full pipeline with DVC:

```powershell
cd mlops
dvc repro
```

This executes:
1. **train** stage â†’ `retrain_model.py`
2. **evaluate** stage â†’ `evaluate_model.py`

Outputs are tracked and versioned automatically.

---

## ğŸ¤– CI/CD Automation

### GitHub Actions Workflow

**File:** `.github/workflows/mlops.yml`

**Triggers:**
- Push to `mlops/data/**` (new dataset)
- Manual workflow dispatch

**Steps:**
1. Install Python dependencies
2. Run retraining pipeline
3. Run monitoring (drift detection)
4. Run evaluation

**To trigger manually:**
- Go to GitHub â†’ Actions â†’ "MLOps Pipeline" â†’ "Run workflow"

---

## ğŸ“ˆ MLflow Experiment Tracking

### Logged Metrics

For each run, MLflow tracks:
- **accuracy** - Overall correctness
- **precision** - Positive predictive value
- **recall** - True positive rate
- **f1** - Harmonic mean of precision/recall
- **roc_auc** - Area under ROC curve

### Logged Parameters

- `algorithm` - Model type (e.g., "keras_dnn")
- `batch_size` - Training batch size
- `epochs` - Number of training epochs
- `type` - Run type (e.g., "production_eval")

### Logged Artifacts

- `metrics` - JSON file with detailed metrics
- `model` - Candidate model directory (if improved)
- `eval` - Evaluation report

### View Experiments

```powershell
mlflow ui --backend-store-uri file:./mlops/tracking --port 5001
```

Navigate to:
- **Experiments** â†’ `LegalAid_Connect_Matchmaking`
- Compare runs, view metrics, download models

---

## ğŸ” Model Promotion Workflow

### Current Process (Manual)

1. Run retraining: `python mlops/scripts/retrain_model.py`
2. Check output: "Improved: True/False"
3. If improved:
   - Review MLflow metrics
   - Test candidate model: `mlops/models/v<timestamp>/lawyer_match_model.h5`
   - Manually copy to production:
     ```powershell
     Copy-Item "mlops/models/v<timestamp>/lawyer_match_model.h5" -Destination "ml_model/lawyer_match_model.h5" -Force
     ```
4. Restart ML service to load new model

### Automated Promotion (Future Enhancement)

Create `mlops/scripts/promote_model.py`:

```python
import shutil
import os
from mlops.scripts.utils import load_production_model, evaluate_model

# Load candidate and production models
# Compare on holdout set
# If candidate wins by threshold (e.g., +2% accuracy):
#   - Backup current production model
#   - Copy candidate to ml_model/
#   - Restart Flask service
```

---

## ğŸ“Š Monitoring Dashboard (Optional)

### Option 1: MLflow UI (Local)

```powershell
mlflow ui --backend-store-uri file:./mlops/tracking --port 5001
```

**Features:**
- Experiment comparison
- Metric visualization
- Model registry
- Artifact browser

### Option 2: Deploy MLflow Server (Cloud)

Deploy to Render, Railway, or AWS:

```yaml
# render.yaml example
services:
  - type: web
    name: mlflow-server
    env: python
    buildCommand: pip install mlflow
    startCommand: mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./artifacts --host 0.0.0.0 --port 5000
```

Then update `mlops/mlflow_setup.py`:

```python
def get_tracking_uri() -> str:
    return os.getenv("MLFLOW_TRACKING_URI", "file:./mlops/tracking")
```

Set environment variable:
```powershell
$env:MLFLOW_TRACKING_URI = "https://your-mlflow-server.onrender.com"
```

---

## ğŸ› ï¸ Utilities Reference

### `mlops/scripts/utils.py`

**Functions:**
- `load_dataset()` - Load training data from `mlops/data/` or fallback to `ml_model/`
- `feature_engineer(df)` - Apply feature engineering (same as production)
- `load_production_model()` - Load model, encoder, scaler from `ml_model/`
- `prepare_xy(df, encoder, scaler)` - Preprocess data for model input
- `evaluate_model(model, X, y)` - Calculate metrics (accuracy, precision, recall, F1, ROC AUC)
- `save_json(data, path)` - Save JSON with metrics/logs

---

## ğŸ” Best Practices

### Data Management

1. **Never modify production data** - MLOps reads from `ml_model/` but doesn't write
2. **Version datasets** - Use DVC to track `mlops/data/matchmaking_dataset.csv`
3. **Snapshot live data** - Periodically export predictions to `mlops/data/current_data.csv` for drift monitoring

### Model Management

1. **Semantic versioning** - Use timestamps: `v20251108_153000`
2. **Keep production stable** - Only promote after thorough testing
3. **Backup before promotion** - Save old model as `lawyer_match_model_backup.h5`

### Experiment Tracking

1. **Meaningful run names** - Use descriptive names: `retrain_20251108`, `eval_production`
2. **Tag experiments** - Add tags for dataset version, feature changes
3. **Document changes** - Log parameter changes in MLflow

### Monitoring

1. **Regular drift checks** - Run weekly or after major data updates
2. **Set drift thresholds** - Define acceptable drift levels (e.g., <10% feature drift)
3. **Alert on degradation** - Monitor accuracy drops >5%

---

## ğŸ› Troubleshooting

### Issue: "Model not found"

**Solution:** Ensure production model exists:
```powershell
Test-Path ml_model/lawyer_match_model.h5
```

### Issue: "No module named 'mlflow'"

**Solution:** Install MLOps dependencies:
```powershell
pip install -r mlops/requirements.txt
```

### Issue: "DVC not initialized"

**Solution:** Initialize DVC in mlops directory:
```powershell
cd mlops
dvc init
cd ..
```

### Issue: "Cannot import from mlops"

**Solution:** Run scripts from project root:
```powershell
# Correct (from project root)
python mlops/scripts/retrain_model.py

# Incorrect (from mlops/)
cd mlops
python scripts/retrain_model.py  # Will fail
```

---

## ğŸ“ Maintenance Schedule

### Daily
- âœ… Check MLflow UI for failed runs
- âœ… Monitor production model accuracy

### Weekly
- âœ… Run drift monitoring: `python mlops/scripts/monitor_model.py`
- âœ… Review drift reports in `mlops/reports/`

### Monthly
- âœ… Retrain model: `python mlops/scripts/retrain_model.py`
- âœ… Compare candidate vs. production
- âœ… Promote if improved by >2%

### Quarterly
- âœ… Review all experiments in MLflow
- âœ… Archive old model versions
- âœ… Update DVC dataset snapshots

---

## ğŸš€ Future Enhancements

### Phase 1 (Current)
- âœ… MLflow tracking
- âœ… DVC data versioning
- âœ… Retraining pipeline
- âœ… Drift monitoring
- âœ… GitHub Actions CI

### Phase 2 (Planned)
- â³ Automated model promotion
- â³ A/B testing framework
- â³ Real-time monitoring dashboard
- â³ Slack/email alerts on drift
- â³ Model explainability (SHAP/LIME)

### Phase 3 (Advanced)
- â³ Multi-model ensemble
- â³ AutoML integration
- â³ Feature store
- â³ Online learning pipeline
- â³ Model serving with Seldon/KServe

---

## ğŸ“š Resources

### Documentation
- [MLflow Docs](https://mlflow.org/docs/latest/index.html)
- [DVC Docs](https://dvc.org/doc)
- [Evidently Docs](https://docs.evidentlyai.com/)

### Tutorials
- [MLflow Tracking](https://mlflow.org/docs/latest/tracking.html)
- [DVC Get Started](https://dvc.org/doc/start)
- [Evidently Tutorials](https://docs.evidentlyai.com/user-guide/tutorials)

---

## ğŸ¤ Contributing

To add new MLOps features:

1. Create feature in `mlops/scripts/`
2. Update `mlops/dvc.yaml` if needed
3. Add to GitHub Actions workflow
4. Document in this README
5. Test without modifying production code

---

## ğŸ“ Support

For MLOps-related issues:
- Check `mlops/tracking/` logs
- Review MLflow UI for run details
- Consult drift reports in `mlops/reports/`

**Remember:** This MLOps layer is **non-intrusive** and operates independently from production. Your main application remains unaffected.

---

**Status:** âœ… MLOps Pipeline Active  
**Last Updated:** November 8, 2025  
**Version:** 1.0.0
